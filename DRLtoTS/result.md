INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 0
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 1 _ 121
INFO     [environment.py:131] Action prop: [-0.0043135   0.3238764   0.07943399]
INFO     [environment.py:132] (3, 32, 8), 2
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1465.5462359685112, 'date': 20130103.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 2 _ 121
INFO     [environment.py:131] Action prop: [-0.0092361   0.19893026  0.10584959]
INFO     [environment.py:132] (3, 32, 8), 3
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1396.6706997380168, 'date': 20130104.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 3 _ 121
INFO     [environment.py:131] Action prop: [0.00492976 0.2028546  0.10089398]
INFO     [environment.py:132] (3, 32, 8), 4
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1380.9035308957143, 'date': 20130105.0, 'loss': tensor(-910.9401, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 4 _ 121
INFO     [environment.py:131] Action prop: [-0.09987032  0.29959154  0.10677192]
INFO     [environment.py:132] (3, 32, 8), 5
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1493.768889888307, 'date': 20130106.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 5 _ 121
INFO     [environment.py:131] Action prop: [0.01077765 0.31347707 0.09937559]
INFO     [environment.py:132] (3, 32, 8), 6
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1376.5932444717837, 'date': 20130107.0, 'loss': tensor(-791.9167, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 6 _ 121
INFO     [environment.py:131] Action prop: [-0.12530419 -0.08914287 -0.0217494 ]
INFO     [environment.py:132] (3, 32, 8), 7
INFO     [environment.py:135] Reward: [10, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([  9.9000, -99.0000, -99.0000])
INFO     [environment.py:137] Rewards: tensor([  9.9000, -99.0000, -99.0000])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 0.0, 'date': 20130108.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 7 _ 121
INFO     [environment.py:131] Action prop: [-0.06026309  0.309841    0.10105148]
INFO     [environment.py:132] (3, 32, 8), 8
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1492.54027230531, 'date': 20130109.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 8 _ 121
INFO     [environment.py:131] Action prop: [-0.00096583  0.28390482  0.09463979]
INFO     [environment.py:132] (3, 32, 8), 9
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1408.7854136427245, 'date': 20130110.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 9 _ 121
INFO     [environment.py:131] Action prop: [0.00766739 0.30307198 0.10669302]
INFO     [environment.py:132] (3, 32, 8), 10
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1481.276837997128, 'date': 20130111.0, 'loss': tensor(-821.9340, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 1
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 2
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 3
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 4
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 5
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 6
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 7
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 8
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 9
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 10
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 11
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 12
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 13
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 14
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 15
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 16
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 17
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 18
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 19
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 20
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 21
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 22
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 23
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 24
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 25
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 26
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 27
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 28
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 29
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 30
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 31
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 32
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 33
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 34
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 35
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 36
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 37
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 38
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 39
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 40
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 41
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 42
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 43
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 44
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 45
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 46
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 47
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 48
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 49
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 50
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 51
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 52
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 53
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 54
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 55
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 56
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 57
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 58
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 59
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 60
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 61
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 62
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 63
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 64
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 65
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 66
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 67
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 68
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 69
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 70
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 71
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 72
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 73
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 74
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 75
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 76
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 77
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 78
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 79
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 80
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 81
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 82
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 83
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 84
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 85
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 86
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 87
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 88
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 89
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 90
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 91
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 92
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 93
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 94
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 95
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 96
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 97
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 98
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0
INFO     [<ipython-input-3-be61abc69532>:5] 


Epoch: 99
INFO     [<ipython-input-3-be61abc69532>:10] 
	Step: 0 _ 121
INFO     [environment.py:131] Action prop: [-0.09924816  0.22837074  0.06278896]
INFO     [environment.py:132] (3, 32, 8), 1
INFO     [environment.py:135] Reward: [-100, -100, -100]
INFO     [policy_net.py:48] Discount Return: tensor([-99., -99., -99.])
INFO     [environment.py:137] Rewards: tensor([-99., -99., -99.])
INFO     [<ipython-input-3-be61abc69532>:18] info: {'vtrue': 1440.191285469115, 'date': 20130102.0, 'loss': tensor(nan, grad_fn=<MulBackward0>)}
INFO     [<ipython-input-3-be61abc69532>:20] Done: 0


[nan, nan, nan, -910.9401245117188, nan, -791.9166870117188, nan, nan, nan, -821.9340209960938, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]